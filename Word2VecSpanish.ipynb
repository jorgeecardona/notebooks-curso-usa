{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, Sequential, Model\n",
    "\n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams # function for making ngrams\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(N=1000, n=100):\n",
    "    \n",
    "    m = Sequential()\n",
    "    \n",
    "    m.add(tf.keras.Input(shape=(N, )))\n",
    "    \n",
    "    # Capa escondida\n",
    "    m.add(layers.Dense(n, activation='linear', use_bias=False, input_shape=(N, ), weight_constraings=tf.keras.constraints.UnitNorm(axis=1)))\n",
    "    \n",
    "    # Afterwards, we do automatic shape inference:\n",
    "    m.add(layers.Dense(N, activation='softmax', use_bias=False))\n",
    "    \n",
    "    m.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "    \n",
    "    print(m.summary())\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['data/pg1619.txt', 'data/pg2000.txt']\n",
    "\n",
    "text = \"\"\n",
    "for file in files:\n",
    "    with open(file) as fd:\n",
    "        text += fd.read()\n",
    "\n",
    "punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "text = re.sub(punctuationNoPeriod, \"\", text)\n",
    "text = text.replace('\\n', ' ')\n",
    "\n",
    "# Let's truncate this to just 1000000 words.\n",
    "corpus = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get individual words\n",
    "tokenized = corpus.split()\n",
    "\n",
    "# Count the words\n",
    "words = collections.Counter(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set(tokenized))\n",
    "unique_words_inv = {}\n",
    "for i, w in enumerate(unique_words):\n",
    "    unique_words_inv[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the tokens into indeces.\n",
    "tokenized_index = [unique_words_inv[w] for w in tokenized]\n",
    "\n",
    "# Lista de n-grams\n",
    "esNgrams = ngrams(tokenized, 5)\n",
    "\n",
    "# contar los n-grams\n",
    "esNgramFreq = collections.Counter(esNgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('don', 'quijote', 'de', 'la', 'mancha'), 134),\n",
       " (('caballero', 'de', 'la', 'triste', 'figura'), 33),\n",
       " (('petrarca', 'de', 'remediis', 'utriusque', 'fortunae'), 30),\n",
       " (('la', 'señora', 'dulcinea', 'del', 'toboso'), 24),\n",
       " (('de', 'remediis', 'utriusque', 'fortunae', 'i'), 21),\n",
       " (('el', 'cura', 'y', 'el', 'barbero'), 21),\n",
       " (('en', 'todos', 'los', 'días', 'de'), 21),\n",
       " (('el', 'caballero', 'de', 'la', 'triste'), 21),\n",
       " (('de', 'don', 'quijote', 'de', 'la'), 17),\n",
       " (('caballero', 'don', 'quijote', 'de', 'la'), 17)]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the ten most popular ngrams in this Spanish corpus?\n",
    "esNgramFreq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(tokens, n=3, batch_size=32):\n",
    "    \n",
    "    N = len(unique_words)\n",
    "    \n",
    "    c_range = list(range(-n, 0, 1)) + list(range(1, n+1))\n",
    "    f_range = list(range(n, len(tokens) - n))\n",
    "    \n",
    "    while True:  \n",
    "        \n",
    "        i = 0 \n",
    "        \n",
    "        x_f = np.zeros((batch_size, N))\n",
    "        x_c = np.zeros((batch_size, N))\n",
    "                \n",
    "        y = np.zeros((batch_size, 1))\n",
    "        \n",
    "        f_i_s = []\n",
    "        f_n_s = []\n",
    "        \n",
    "        while i < batch_size:\n",
    "                                    \n",
    "            if len(f_i_s) == 0:\n",
    "                f_i_s = random.choices(f_range, k=100 * batch_size)\n",
    "                                \n",
    "            if len(f_n_s) == 0:\n",
    "                f_n_s = random.choices(f_range, k=100 * batch_size)\n",
    "\n",
    "            # Focus.\n",
    "            f_i = f_i_s.pop()\n",
    "\n",
    "            # Pick another random word.\n",
    "            f_n = f_n_s.pop()\n",
    "            \n",
    "            # Context\n",
    "            for c in c_range:                \n",
    "                \n",
    "                if i >= batch_size:\n",
    "                    break\n",
    "                    \n",
    "                x_f[i][tokens[f_i]] = 1.0\n",
    "                x_c[i][tokens[f_i + c]] = 1.0\n",
    "                y[i][0] = 1.0\n",
    "\n",
    "                i += 1\n",
    "            \n",
    "            # Context\n",
    "            for c in c_range:                \n",
    "                \n",
    "                if i >= batch_size:\n",
    "                    break\n",
    "                    \n",
    "                x_f[i][tokens[f_n]] = 1.0\n",
    "                x_c[i][tokens[f_i + c]] = 1.0\n",
    "                y[i][0] = 0.0\n",
    "                i += 1            \n",
    "        \n",
    "        yield ((x_f, x_c), y)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_66 (InputLayer)           [(None, 35200)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_67 (InputLayer)           [(None, 35200)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "w_r (Dense)                     (None, 100)          3520000     input_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "w_c (Dense)                     (None, 100)          3520000     input_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.tensordot_21 (TFOpLambda)    (None, None)         0           w_r[0][0]                        \n",
      "                                                                 w_c[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.linalg.diag_part_7 (TFOpLamb (None,)              0           tf.tensordot_21[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_16 (TFOpLambda)      (None, 1)            0           tf.linalg.diag_part_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_63 (Dense)                (None, 1)            1           tf.reshape_16[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 7,040,001\n",
      "Trainable params: 7,040,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_model(N=1000, n=1000):\n",
    "    \n",
    "    # Focus word.\n",
    "    focus = tf.keras.Input(shape=(N, ))\n",
    "    \n",
    "    # Context word.\n",
    "    context = tf.keras.Input(shape=(N, ))\n",
    "    \n",
    "    # Matrix with repr and context.\n",
    "    w_r = layers.Dense(n, activation='linear', use_bias=False, kernel_constraint=tf.keras.constraints.UnitNorm(axis=1), name='w_r')    \n",
    "    w_c = layers.Dense(n, activation='linear', use_bias=False, kernel_constraint=tf.keras.constraints.UnitNorm(axis=1), name='w_c')\n",
    "    \n",
    "    # Dot product.\n",
    "    y_ = tf.linalg.diag_part(tf.tensordot(w_r(focus), w_c(context), [[1],[1]]))\n",
    "    y = tf.reshape(y_, [-1, 1])\n",
    "    \n",
    "    output = layers.Dense(1, activation='sigmoid', use_bias=False)(y)\n",
    "\n",
    "    m = Model(inputs=[focus, context], outputs=output)\n",
    "    \n",
    "    m.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    print(m.summary())\n",
    "    \n",
    "    return m\n",
    "    \n",
    "\n",
    "m = build_model(N=len(unique_words), n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "20/20 [==============================] - 3s 130ms/step - loss: 0.6927\n",
      "Epoch 2/10000\n",
      "20/20 [==============================] - 2s 114ms/step - loss: 0.6931\n",
      "Epoch 3/10000\n",
      "20/20 [==============================] - 2s 112ms/step - loss: 0.6939\n",
      "Epoch 4/10000\n",
      "20/20 [==============================] - 3s 128ms/step - loss: 0.6932\n",
      "Epoch 5/10000\n",
      "20/20 [==============================] - 2s 107ms/step - loss: 0.6932 \n",
      "Epoch 6/10000\n",
      "20/20 [==============================] - 2s 116ms/step - loss: 0.6931\n",
      "Epoch 7/10000\n",
      "20/20 [==============================] - 3s 128ms/step - loss: 0.6926\n",
      "Epoch 8/10000\n",
      "17/20 [========================>.....] - ETA: 0s - loss: 0.6934"
     ]
    }
   ],
   "source": [
    "m.fit(train_generator(tokenized_index, batch_size=256), steps_per_epoch=20, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_d(w):\n",
    "    W_r = np.array(m.get_layer('w_r').weights[0])\n",
    "    return W_r[unique_words_inv[w]]\n",
    "\n",
    "\n",
    "def closest(w, n=10):\n",
    "    \n",
    "    W_r = np.array(m.get_layer('w_r').weights[0])\n",
    "    \n",
    "    w = w / np.linalg.norm(w)\n",
    "    \n",
    "    sort_index = np.flip(np.dot(W_r, w).argsort())\n",
    "    \n",
    "    words = []\n",
    "    for i in range(n):\n",
    "        words.append(unique_words[sort_index[i]])\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09154004, -0.0746439 , -0.07706562, -0.15104972, -0.20269592,\n",
       "       -0.05375413,  0.09674148,  0.05664596,  0.03870582,  0.02036014,\n",
       "       -0.05095919, -0.251195  ,  0.00127836, -0.21020204,  0.07367929,\n",
       "       -0.06879904,  0.14934994,  0.07993663,  0.05136886, -0.02097218,\n",
       "        0.08394454, -0.02790381,  0.04458955, -0.10188736,  0.10768676,\n",
       "        0.00679626, -0.16269545,  0.07203142, -0.00715161, -0.09431475,\n",
       "       -0.02478926,  0.0222859 , -0.03121722, -0.05354659, -0.02604924,\n",
       "        0.00080278, -0.00987856,  0.00096193, -0.11782497, -0.00272015,\n",
       "       -0.10738652, -0.11440382, -0.09793027, -0.03792438,  0.02099987,\n",
       "       -0.06834219,  0.00841567, -0.17905857, -0.08828238,  0.10164704,\n",
       "       -0.03792677,  0.06343715,  0.04233918, -0.09589901, -0.05871633,\n",
       "        0.02461573, -0.1306635 ,  0.09981474, -0.20335914, -0.20875572,\n",
       "        0.14069352,  0.15361135, -0.03220928,  0.0077925 ,  0.0203077 ,\n",
       "        0.0426612 ,  0.23812217, -0.02554249, -0.10857997,  0.00581171,\n",
       "       -0.15451346,  0.1316106 ,  0.06722184, -0.00400837,  0.09317543,\n",
       "        0.00134398, -0.00762463, -0.08345542,  0.06531011,  0.04090384,\n",
       "        0.02320162,  0.09148171, -0.08044866, -0.0224262 ,  0.10221314,\n",
       "        0.04597939, -0.02266087,  0.07335002, -0.06144604, -0.01081858,\n",
       "        0.23193035,  0.08896887, -0.1542149 , -0.10489798,  0.28944302,\n",
       "        0.08395737,  0.01083053, -0.01153759, -0.08445063, -0.12351454],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_d('mujer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rey',\n",
       " 'mujer',\n",
       " 'tierra',\n",
       " 'sombra',\n",
       " 'belleza',\n",
       " 'contentísimas',\n",
       " 'capirotes',\n",
       " 'edad',\n",
       " 'mitad',\n",
       " 'cumbre']"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(phi_d('rey') - phi_d('hombre') + phi_d('mujer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['verde',\n",
       " 'vestida',\n",
       " 'vestido',\n",
       " 'pastor',\n",
       " '195',\n",
       " 'complexión',\n",
       " 'industriadas',\n",
       " 'estudiante',\n",
       " 'lusitano',\n",
       " 'principales']"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(phi_d('verde'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words['reina']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
