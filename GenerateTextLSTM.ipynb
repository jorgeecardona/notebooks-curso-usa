{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import string\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "\n",
    "if len(gpus) > 0:\n",
    "    tf.config.experimental.set_visible_devices(devices=gpus[0], device_type='GPU')\n",
    "    tf.config.experimental.set_memory_growth(device=gpus[0], enable=True)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend, losses, Sequential\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['data/pg1619.txt', 'data/pg2000.txt']\n",
    "\n",
    "text = \"\"\n",
    "for file in files:\n",
    "    with open(file) as fd:\n",
    "        text += fd.read()\n",
    "        \n",
    "text = text[:1000000]\n",
    "        \n",
    "punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "text = re.sub(punctuationNoPeriod, \"\", text)\n",
    "text = text.replace('\\n', ' ')\n",
    "\n",
    "# Let's truncate this to just 1000000 words.\n",
    "corpus = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get individual words\n",
    "tokenized = corpus.split()\n",
    "\n",
    "# Count the words\n",
    "words = collections.Counter(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set(tokenized))\n",
    "unique_words_inv = {}\n",
    "for i, w in enumerate(unique_words):\n",
    "    unique_words_inv[w] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the tokens into indeces.\n",
    "tokenized_index = [unique_words_inv[w] for w in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         2195300   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 8)                 3488      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 21953)             724449    \n",
      "=================================================================\n",
      "Total params: 2,923,925\n",
      "Trainable params: 2,923,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "\n",
    "    m = Sequential()\n",
    "    \n",
    "    m.add(layers.Embedding(len(unique_words), 100, input_shape=(None, )))\n",
    "    \n",
    "    m.add(layers.LSTM(8))\n",
    "    \n",
    "    m.add(layers.Dense(16, activation='relu'))\n",
    "    \n",
    "    m.add(layers.Dense(32, activation='relu'))\n",
    "    \n",
    "    m.add(layers.Dense(len(unique_words), activation='softmax'))\n",
    "    \n",
    "    m.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    \n",
    "    print(m.summary())\n",
    "    \n",
    "    return m\n",
    "    \n",
    "m = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(tokens, batch_size=32):\n",
    "    \n",
    "    j_ranges = []\n",
    "    for n in range(1, 30):\n",
    "        j_ranges.append(list(range(n, len(tokens))))\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        n = random.randint(1, 29)\n",
    "    \n",
    "        x = np.zeros((batch_size, n))\n",
    "        y = np.zeros((batch_size, len(unique_words)))\n",
    "    \n",
    "        for i, j in enumerate(random.choices(j_ranges[n-1], k=batch_size)):\n",
    "            x[i][:] = np.array(tokens[j-n:j], dtype='int')\n",
    "            y[i][tokens[j]] = 1.0\n",
    "        \n",
    "        yield (x.reshape((batch_size, n)), y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "100/100 [==============================] - 3s 29ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.2194\n",
      "Epoch 2/10000\n",
      "100/100 [==============================] - 3s 30ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.2455\n",
      "Epoch 3/10000\n",
      "100/100 [==============================] - 3s 33ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.1877\n",
      "Epoch 4/10000\n",
      "100/100 [==============================] - 3s 31ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.2308\n",
      "Epoch 5/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.2344\n",
      "Epoch 6/10000\n",
      "100/100 [==============================] - 3s 31ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.1878\n",
      "Epoch 7/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.1845\n",
      "Epoch 8/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.2566\n",
      "Epoch 9/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.1601\n",
      "Epoch 10/10000\n",
      "100/100 [==============================] - 3s 33ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.1104\n",
      "Epoch 11/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.1134\n",
      "Epoch 12/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0831\n",
      "Epoch 13/10000\n",
      "100/100 [==============================] - 3s 33ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0878\n",
      "Epoch 14/10000\n",
      "100/100 [==============================] - 3s 33ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0440\n",
      "Epoch 15/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0809\n",
      "Epoch 16/10000\n",
      "100/100 [==============================] - 3s 33ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0882\n",
      "Epoch 17/10000\n",
      "100/100 [==============================] - 3s 33ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0276\n",
      "Epoch 18/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0793\n",
      "Epoch 19/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0758\n",
      "Epoch 20/10000\n",
      "100/100 [==============================] - 3s 33ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0194\n",
      "Epoch 21/10000\n",
      "100/100 [==============================] - 3s 34ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0827\n",
      "Epoch 22/10000\n",
      "100/100 [==============================] - 3s 32ms/step - batch: 49.5000 - size: 64.0000 - loss: 6.9861\n",
      "Epoch 23/10000\n",
      "100/100 [==============================] - 3s 30ms/step - batch: 49.5000 - size: 64.0000 - loss: 6.9865\n",
      "Epoch 24/10000\n",
      "100/100 [==============================] - 3s 28ms/step - batch: 49.5000 - size: 64.0000 - loss: 7.0343\n",
      "Epoch 25/10000\n",
      "100/100 [==============================] - 3s 28ms/step - batch: 49.5000 - size: 64.0000 - loss: 6.9413\n",
      "Epoch 26/10000\n",
      "100/100 [==============================] - 3s 28ms/step - batch: 49.5000 - size: 64.0000 - loss: 6.9402\n",
      "Epoch 27/10000\n",
      " 13/100 [==>...........................] - ETA: 2s - batch: 6.0000 - size: 64.0000 - loss: 7.1667"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f0c2602c4de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m     return func.fit(\n\u001b[0m\u001b[1;32m    797\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    575\u001b[0m     training_utils_v1.check_generator_arguments(\n\u001b[1;32m    576\u001b[0m         y, sample_weight, validation_split=validation_split)\n\u001b[0;32m--> 577\u001b[0;31m     return fit_generator(\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_generator_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1066\u001b[0m                                 'distributed with tf.distribute.Strategy.')\n\u001b[1;32m   1067\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m     x, y, sample_weights = self._standardize_user_data(\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         extract_tensors_from_dataset=True)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m     \u001b[0;31m# self.run_eagerly is not free to compute, so we want to reuse the value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m     \u001b[0mrun_eagerly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m     if (not run_eagerly and is_build_called and is_compile_called and\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mrun_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    552\u001b[0m       raise ValueError('You can only set `run_eagerly=True` if eager execution '\n\u001b[1;32m    553\u001b[0m                        'is enabled.')\n\u001b[0;32m--> 554\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# Respect `tf.config.run_functions_eagerly` unless\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36mdynamic\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m.fit(train_generator(tokenized_index, batch_size=64), steps_per_epoch=100, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(w):\n",
    "    print(\" \".join(w))\n",
    "    x = np.array([unique_words_inv[_] for _ in w], dtype='int').reshape((1, len(w)))\n",
    "    \n",
    "    y = m.predict(x)\n",
    "    y = np.reshape(y, (y.size, ))\n",
    "    \n",
    "    k = y.argmax()\n",
    "    \n",
    "    return unique_words[k]    \n",
    "\n",
    "def next_words(w, n):\n",
    "    w = w.copy()\n",
    "    print(\" \".join(w))\n",
    "    for i in range(n):\n",
    "        w.append(next_word(w))\n",
    "        \n",
    "    print(\" \".join(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'de'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word(tokenized[200:210])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que que que que que que que que que\n",
      "dos personajes centrales cuyas vidas y destinos se enlazan apasionadamente de que que que que que que que que que que que que que que que que que que que que que que que que que que que que que\n"
     ]
    }
   ],
   "source": [
    "next_words(tokenized[200:210], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'hola' in unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
